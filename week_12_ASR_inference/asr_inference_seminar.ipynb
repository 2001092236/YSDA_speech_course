{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08ab13cf-c2d5-4620-aae1-debd798e2b1a",
   "metadata": {},
   "source": [
    "Today we will be implementing chunked-streaming for ctc-conformer model\n",
    "\n",
    "Recap:\n",
    "\n",
    "* [Conformer](https://arxiv.org/pdf/2005.08100) \n",
    "* [Chunked streaming](https://arxiv.org/pdf/2312.17279)\n",
    "* [NeMo repository](https://github.com/NVIDIA/NeMo) -- source of weight and basic idea for our seminar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa34252-b022-4873-8332-1fa18d2be3b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c1c2c7-af40-490d-8ec8-65eefc017ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c69ecec-2c8e-4bc2-a482-93a352a52103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install librosa\n",
    "# !pip install torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280ea9e9-a4b3-41ea-b41c-8fc349bb48c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import librosa\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import queue\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "import wave\n",
    "\n",
    "from IPython.display import Audio\n",
    "from urllib.parse import urlencode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68f7ecf-8223-43b6-a276-2474d07a0cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(public_link, filename='archieve.tgz'):\n",
    "    base_url = 'https://cloud-api.yandex.net/v1/disk/public/resources/download?'\n",
    "    final_url = base_url + urlencode(dict(public_key=public_link))\n",
    "    response = requests.get(final_url)\n",
    "    parse_href = response.json()['href']\n",
    "\n",
    "    url = parse_href\n",
    "    download_url = requests.get(url)\n",
    "    final_link = os.path.join(os.getcwd(), filename)\n",
    "    print(final_link)\n",
    "    with open(final_link, 'wb') as ff:\n",
    "        ff.write(download_url.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed33f4-aa3b-43db-9cd1-2c48aa5fbd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# link_to_archive = \"https://disk.yandex.ru/d/Omgg4HryF5AWLQ\"\n",
    "# download_file(link_to_archive, filename='archieve.tgz')\n",
    "# !mkdir -p ../data\n",
    "# !mv archieve.tgz ../data/\n",
    "# !tar xzvf ../data/archieve.tgz -C ../data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a1759-8774-4a38-94d5-26bd3103437c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a6d1ca-37b7-4067-8a80-66e3ee5848bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilterbankFeatures(torch.nn.Module):\n",
    "    \"\"\"Featurizer that converts wavs to Mel Spectrograms.\n",
    "    See AudioToMelSpectrogramPreprocessor for args.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_rate=16000,\n",
    "        n_window_size=400,\n",
    "        n_window_stride=160,\n",
    "        # n_fft=512,\n",
    "        preemph=0.97,\n",
    "        nfilt=80,\n",
    "        lowfreq=0,\n",
    "        highfreq=None,\n",
    "        log=True,\n",
    "        log_zero_guard_value=2 ** -24,\n",
    "        pad_value=0,\n",
    "        nb_max_freq=4000,\n",
    "        mel_norm=\"slaney\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.log_zero_guard_value = log_zero_guard_value\n",
    "\n",
    "        self.win_length = n_window_size\n",
    "        self.hop_length = n_window_stride\n",
    "        self.n_fft = n_window_size\n",
    "\n",
    "        window_fn = torch.hann_window\n",
    "        window_tensor = window_fn(self.win_length, periodic=False)\n",
    "        self.register_buffer(\"window\", window_tensor)\n",
    "        self.stft = lambda x: torch.stft(\n",
    "            x,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.hop_length,\n",
    "            win_length=self.win_length,\n",
    "            center=True,\n",
    "            window=self.window.to(dtype=torch.float),\n",
    "            return_complex=True,\n",
    "        )\n",
    "\n",
    "        self.nfilt = nfilt\n",
    "        self.preemph = preemph\n",
    "        highfreq = highfreq or sample_rate / 2\n",
    "\n",
    "        filterbanks = torch.tensor(\n",
    "            librosa.filters.mel(\n",
    "                sr=sample_rate, n_fft=self.n_fft, n_mels=nfilt, fmin=lowfreq, fmax=highfreq, norm=mel_norm\n",
    "            ),\n",
    "            dtype=torch.float,\n",
    "        ).unsqueeze(0)\n",
    "        self.register_buffer(\"fb\", filterbanks)\n",
    "\n",
    "        self.pad_value = pad_value\n",
    "\n",
    "        self.forward = torch.no_grad()(self.forward)\n",
    "\n",
    "    @property\n",
    "    def filter_banks(self):\n",
    "        return self.fb\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat((x[0].unsqueeze(0), x[1:] - self.preemph * x[:-1]), dim=0)\n",
    "        # disable autocast to get full range of stft values\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            x = self.stft(x)\n",
    "        x = torch.view_as_real(x)\n",
    "        x = x.pow(2).sum(-1)\n",
    "        # dot with filterbank energies\n",
    "        x = torch.matmul(self.fb.to(x.dtype), x).squeeze(0)\n",
    "        \n",
    "        x = torch.log(x + self.log_zero_guard_value)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55843831-ea15-4615-8ec7-6a515bfffc83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4445ec2-ed50-4cae-aaa1-c52ca4ada53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelPositionalEncoding(torch.nn.Module):\n",
    "    \"\"\"Relative positional encoding for TransformerXL's layers\n",
    "    See : Appendix B in https://arxiv.org/abs/1901.02860\n",
    "    Args:\n",
    "        d_model (int): embedding dim\n",
    "        dropout_rate (float): dropout rate\n",
    "        max_len (int): maximum input length\n",
    "        xscale (bool): whether to scale the input by sqrt(d_model)\n",
    "        dropout_rate_emb (float): dropout rate for the positional embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout_rate, max_len=5000, xscale=None, dropout_rate_emb=0.0):\n",
    "        \"\"\"Construct an PositionalEncoding object.\"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.xscale = xscale\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "        self.max_len = max_len\n",
    "        if dropout_rate_emb > 0:\n",
    "            self.dropout_emb = torch.nn.Dropout(dropout_rate_emb)\n",
    "        else:\n",
    "            self.dropout_emb = None\n",
    "\n",
    "    def create_pe(self, positions):\n",
    "        pos_length = positions.size(0)\n",
    "        pe = torch.zeros(pos_length, self.d_model, device=positions.device)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, self.d_model, 2, dtype=torch.float32, device=positions.device)\n",
    "            * -(math.log(10000.0) / self.d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(positions * div_term)\n",
    "        pe[:, 1::2] = torch.cos(positions * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        if hasattr(self, 'pe'):\n",
    "            self.pe = pe\n",
    "        else:\n",
    "            self.register_buffer('pe', pe, persistent=False)\n",
    "\n",
    "    def extend_pe(self, length, device=None):\n",
    "        \"\"\"Reset and extend the positional encodings if needed.\"\"\"\n",
    "        needed_size = 2 * length - 1\n",
    "        if hasattr(self, 'pe') and self.pe.size(1) >= needed_size:\n",
    "            return\n",
    "        # positions would be from negative numbers to positive\n",
    "        # positive positions would be used for left positions and negative for right positions\n",
    "        positions = torch.arange(length - 1, -length, -1, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        self.create_pe(positions=positions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Compute positional encoding.\n",
    "        Args:\n",
    "            x (torch.Tensor): Input. Its shape is (batch, time, feature_size)\n",
    "        Returns:\n",
    "            x (torch.Tensor): Its shape is (batch, time, feature_size)\n",
    "            pos_emb (torch.Tensor): Its shape is (1, 2 * time - 1, feature_size)\n",
    "        \"\"\"\n",
    "\n",
    "        if self.xscale:\n",
    "            x = x * self.xscale\n",
    "\n",
    "        # center_pos would be the index of position 0\n",
    "        # negative positions would be used for right and positive for left tokens\n",
    "        # for input of length L, 2*L-1 positions are needed, positions from (L-1) to -(L-1)\n",
    "        input_len = x.size(1)\n",
    "        center_pos = self.pe.size(1) // 2 + 1\n",
    "        start_pos = center_pos - input_len\n",
    "        end_pos = center_pos + input_len - 1\n",
    "        pos_emb = self.pe[:, start_pos:end_pos]\n",
    "        if self.dropout_emb:\n",
    "            pos_emb = self.dropout_emb(pos_emb)\n",
    "        return self.dropout(x), pos_emb\n",
    "\n",
    "    def get_initial_state(self, chunk_size, left_context_chunks):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def streaming_forward(self, x, state):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a9ef3f-c4ad-484c-b87e-90cc461f4277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attn_mask(chunk_size: int, left_chunks_num: int, max_length: int, device):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        torch.Tensor (1, max_length, max_length) [bool]: True means value should be used.\n",
    "    \"\"\"\n",
    "    # [t]\n",
    "    chunk_idx = torch.arange(0, max_length, dtype=torch.int, device=device)\n",
    "    chunk_idx = torch.div(chunk_idx, chunk_size, rounding_mode=\"trunc\")\n",
    "\n",
    "    # [t, t]: diff_chunks[i, j] = chunk_idx[i] - chunk_idx[j]\n",
    "    diff_chunks = chunk_idx.unsqueeze(1) - chunk_idx.unsqueeze(0)\n",
    "    chunked_limited_mask = torch.logical_and(\n",
    "        torch.le(diff_chunks, left_chunks_num), torch.ge(diff_chunks, 0)\n",
    "    )\n",
    "    att_mask = chunked_limited_mask.unsqueeze(0)\n",
    "    return att_mask\n",
    "\n",
    "\n",
    "class RelPositionMultiHeadAttention(torch.nn.Module):\n",
    "    \"\"\"Multi-Head Attention layer of Transformer-XL with support of relative positional encoding.\n",
    "    Paper: https://arxiv.org/abs/1901.02860\n",
    "    Args:\n",
    "        n_head (int): number of heads\n",
    "        n_feat (int): size of the features\n",
    "        dropout_rate (float): dropout rate\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_head: int,\n",
    "        n_feat: int,\n",
    "        dropout_rate: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert n_feat % n_head == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = n_feat // n_head\n",
    "        self.s_d_k = math.sqrt(self.d_k)\n",
    "        self.h = n_head\n",
    "        self.linear_q = torch.nn.Linear(n_feat, n_feat)\n",
    "        self.linear_k = torch.nn.Linear(n_feat, n_feat)\n",
    "        self.linear_v = torch.nn.Linear(n_feat, n_feat)\n",
    "        self.linear_out = torch.nn.Linear(n_feat, n_feat)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        # linear transformation for positional encoding\n",
    "        self.linear_pos = torch.nn.Linear(n_feat, n_feat, bias=False)\n",
    "\n",
    "        # self.pos_bias_u = pos_bias_u\n",
    "        # self.pos_bias_v = pos_bias_v\n",
    "        self.pos_bias_u = torch.nn.Parameter(torch.FloatTensor(self.h, self.d_k))\n",
    "        self.pos_bias_v = torch.nn.Parameter(torch.FloatTensor(self.h, self.d_k))\n",
    "\n",
    "        torch.nn.init.zeros_(self.pos_bias_u)\n",
    "        torch.nn.init.zeros_(self.pos_bias_v)\n",
    "\n",
    "    def forward_qkv(self, query, key, value):\n",
    "        \"\"\"Transforms query, key and value.\n",
    "        Args:\n",
    "            query (torch.Tensor): (batch, time1, size)\n",
    "            key (torch.Tensor): (batch, time2, size)\n",
    "            value (torch.Tensor): (batch, time2, size)\n",
    "        returns:\n",
    "            q (torch.Tensor): (batch, head, time1, size)\n",
    "            k (torch.Tensor): (batch, head, time2, size)\n",
    "            v (torch.Tensor): (batch, head, time2, size)\n",
    "        \"\"\"\n",
    "        n_batch = query.size(0)\n",
    "        q = self.linear_q(query).view(n_batch, -1, self.h, self.d_k)\n",
    "        k = self.linear_k(key).view(n_batch, -1, self.h, self.d_k)\n",
    "        v = self.linear_v(value).view(n_batch, -1, self.h, self.d_k)\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        return q, k, v\n",
    "\n",
    "    def forward_attention(self, value, scores, mask):\n",
    "        \"\"\"Compute attention context vector.\n",
    "        Args:\n",
    "            value (torch.Tensor): (batch, time2, size)\n",
    "            scores(torch.Tensor): (batch, time1, time2)\n",
    "            mask(torch.Tensor): (batch, time1, time2)\n",
    "        returns:\n",
    "            value (torch.Tensor): transformed `value` (batch, time2, d_model) weighted by the attention scores\n",
    "        \"\"\"\n",
    "        n_batch = value.size(0)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)  # (batch, 1, time1, time2)\n",
    "            scores = scores.masked_fill(mask, -10000.0)\n",
    "            attn = torch.softmax(scores, dim=-1).masked_fill(mask, 0.0)  # (batch, head, time1, time2)\n",
    "        else:\n",
    "            attn = torch.softmax(scores, dim=-1)  # (batch, head, time1, time2)\n",
    "\n",
    "        p_attn = self.dropout(attn)\n",
    "        x = torch.matmul(p_attn, value)  # (batch, head, time1, d_k)\n",
    "        x = x.transpose(1, 2).reshape(n_batch, -1, self.h * self.d_k)  # (batch, time1, d_model)\n",
    "\n",
    "        return self.linear_out(x)  # (batch, time1, d_model)\n",
    "    \n",
    "    def rel_shift(self, x):\n",
    "        \"\"\"Compute relative positional encoding.\n",
    "        Args:\n",
    "            x (torch.Tensor): (batch, nheads, time1, 2*time2-1)\n",
    "        \"\"\"\n",
    "        b, h, qlen, pos_len = x.size()  # (b, h, t1, 2 * t2 - 1)\n",
    "        # need to add a column of zeros on the left side of last dimension to perform the relative shifting\n",
    "        x = torch.nn.functional.pad(x, pad=(1, 0))  # (b, h, t1, 1 + (2 * t2 - 1))\n",
    "        x = x.view(b, h, -1, qlen)  # (b, h, t1 + 1, 2 * t2 - 1)\n",
    "        # need to drop the first row\n",
    "        x = x[:, :, 1:].view(b, h, qlen, pos_len)  # (b, h, t1, 2 * t2 - 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, query, key, value, mask, pos_emb):\n",
    "        \"\"\"Compute 'Scaled Dot Product Attention' with rel. positional encoding.\n",
    "        Args:\n",
    "            query (torch.Tensor): (batch, time1, size)\n",
    "            key (torch.Tensor): (batch, time2, size)\n",
    "            value(torch.Tensor): (batch, time2, size)\n",
    "            mask (torch.Tensor): (batch, time1, time2)\n",
    "            pos_emb (torch.Tensor) : (batch, 2 * time1 - 1, size)\n",
    "\n",
    "        Returns:\n",
    "            output (torch.Tensor): transformed `value` (batch, time1, d_model) weighted by the query dot key attention\n",
    "        \"\"\"\n",
    "\n",
    "        q, k, v = self.forward_qkv(query, key, value)\n",
    "        q = q.transpose(1, 2)  # (batch, time1, head, d_k)\n",
    "\n",
    "        n_batch_pos = pos_emb.size(0)\n",
    "\n",
    "        # (batch, 2 * time1 - 1, head, d_k)\n",
    "        p = self.linear_pos(pos_emb).view(n_batch_pos, -1, self.h, self.d_k)\n",
    "        # (batch, head, 2 * time1 - 1, d_k)\n",
    "        p = p.transpose(1, 2)\n",
    "\n",
    "        # (batch, head, time1, d_k)\n",
    "        q_with_bias_u = (q + self.pos_bias_u).transpose(1, 2)\n",
    "        # (batch, head, time1, d_k)\n",
    "        q_with_bias_v = (q + self.pos_bias_v).transpose(1, 2)\n",
    "\n",
    "        # compute attention score\n",
    "        # first compute matrix a and matrix c\n",
    "        # as described in https://arxiv.org/abs/1901.02860 Section 3.3\n",
    "        # (batch, head, time1, time2)\n",
    "        matrix_ac = torch.matmul(q_with_bias_u, k.transpose(-2, -1))\n",
    "\n",
    "        # compute matrix b and matrix d\n",
    "        # (batch, head, time1, time1)\n",
    "        matrix_bd = torch.matmul(q_with_bias_v, p.transpose(-2, -1))\n",
    "        matrix_bd = self.rel_shift(matrix_bd)\n",
    "\n",
    "        # drops extra elements in the matrix_bd to match the matrix_ac's size\n",
    "        matrix_bd = matrix_bd[:, :, :, : matrix_ac.size(-1)]\n",
    "\n",
    "        scores = (matrix_ac + matrix_bd) / self.s_d_k  # (batch, head, time1, time2)\n",
    "\n",
    "        out = self.forward_attention(v, scores, mask)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def get_initial_state(self, chunk_size, left_context_chunks):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def streaming_forward(self, x, pos_emb, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: torch.Tensor (1, T, d)\n",
    "            pos_emb: ??\n",
    "            state: ???\n",
    "        Returns:\n",
    "            x, state\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2125cf-836c-4ab6-a4d0-121ee51fafbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_length(lengths: torch.Tensor, all_paddings: int, kernel_size: int, stride: int, repeat_num=1):\n",
    "    \"\"\" Calculates the output length of a Tensor passed through a convolution or max pooling layer\"\"\"\n",
    "    add_pad: float = all_paddings - kernel_size\n",
    "    one: float = 1.0\n",
    "    for i in range(repeat_num):\n",
    "        lengths = torch.div(lengths.to(dtype=torch.float) + add_pad, stride) + one\n",
    "        lengths = torch.floor(lengths)\n",
    "    return lengths.to(dtype=torch.int)\n",
    "\n",
    "\n",
    "class CausalConv2D(torch.nn.Conv2d):\n",
    "    \"\"\"\n",
    "    A causal version of nn.Conv2d where each location in the 2D matrix would have no access to locations on its right or down\n",
    "    All arguments are the same as nn.Conv2d except padding which should be set as None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feats: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        stride: int = 1,\n",
    "        groups: int = 1,\n",
    "    ) -> None:\n",
    "        self._in_feats = in_feats\n",
    "        self._in_channels = in_channels\n",
    "\n",
    "        # NOTE: originally (in NeMo repo) right_padding = bottom_padding = stride - 1\n",
    "        # but we change right_padding to 0 for better streaming consistency\n",
    "        # and keep _bottom_padding at stride - 1 to have matching weights shape\n",
    "        self._left_padding = kernel_size - 1\n",
    "        self._right_padding = 0 # stride - 1\n",
    "        self._top_padding = kernel_size - 1\n",
    "        self._bottom_padding = stride - 1\n",
    "\n",
    "        super(CausalConv2D, self).__init__(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=0,\n",
    "            groups=groups,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.pad(x, pad=(self._top_padding, self._bottom_padding, self._left_padding, self._right_padding))\n",
    "        x = super().forward(x)\n",
    "        return x\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def streaming_forward(self, x, state):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "\n",
    "class ConvSubsampling(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        subsampling_factor: int,\n",
    "        feat_in: int,\n",
    "        feat_out: int,\n",
    "        conv_channels: int,\n",
    "        activation,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._conv_channels = conv_channels\n",
    "        self._feat_in = feat_in\n",
    "        self._feat_out = feat_out\n",
    "        self._sampling_num = int(math.log(subsampling_factor, 2))\n",
    "        self._subsampling_factor = subsampling_factor\n",
    "\n",
    "        in_channels = 1\n",
    "        layers = []\n",
    "\n",
    "        self._stride = 2\n",
    "        self._kernel_size = 3\n",
    "\n",
    "        self._left_padding = self._kernel_size - 1\n",
    "        # self._right_padding = self._stride - 1\n",
    "        self._right_padding = 0\n",
    "        self._top_padding = self._kernel_size - 1\n",
    "        self._bottom_padding = self._stride - 1\n",
    "\n",
    "        # Layer 1\n",
    "        \n",
    "        layers.append(\n",
    "            CausalConv2D(\n",
    "                in_feats = self._feat_in,\n",
    "                in_channels=in_channels,\n",
    "                out_channels=conv_channels,\n",
    "                kernel_size=self._kernel_size,\n",
    "                stride=self._stride,\n",
    "            )\n",
    "        )\n",
    "        in_channels = conv_channels\n",
    "        in_feats = int(\n",
    "            calc_length(\n",
    "                torch.tensor(self._feat_in, dtype=torch.float),\n",
    "                all_paddings=self._top_padding + self._bottom_padding,\n",
    "                kernel_size=self._kernel_size,\n",
    "                stride=self._stride,\n",
    "                repeat_num=1\n",
    "            )\n",
    "        )\n",
    "            \n",
    "        layers.append(activation)\n",
    "\n",
    "        for i in range(self._sampling_num - 1):\n",
    "            layers.append(\n",
    "                CausalConv2D(\n",
    "                    in_feats=in_feats,\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels,\n",
    "                    kernel_size=self._kernel_size,\n",
    "                    stride=self._stride,\n",
    "                    groups=in_channels,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            layers.append(\n",
    "                torch.nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=conv_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                    groups=1,\n",
    "                )\n",
    "            )\n",
    "            layers.append(activation)\n",
    "            in_channels = conv_channels\n",
    "            in_feats = int(\n",
    "                calc_length(\n",
    "                    torch.tensor(in_feats, dtype=torch.float),\n",
    "                    all_paddings=self._top_padding + self._bottom_padding,\n",
    "                    kernel_size=self._kernel_size,\n",
    "                    stride=self._stride,\n",
    "                    repeat_num=1\n",
    "                )\n",
    "            )\n",
    "\n",
    "        out_length = calc_length(\n",
    "            lengths=torch.tensor(self._feat_in, dtype=torch.float),\n",
    "            all_paddings=self._top_padding + self._bottom_padding,\n",
    "            kernel_size=self._kernel_size,\n",
    "            stride=self._stride,\n",
    "            repeat_num=self._sampling_num,\n",
    "        )\n",
    "\n",
    "        # b, c, t, f -> b, t, feat_out\n",
    "        self.out = torch.nn.Linear(conv_channels * out_length, self._feat_out)\n",
    "        self.conv = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        lengths = calc_length(\n",
    "            lengths,\n",
    "            all_paddings=self._left_padding + self._right_padding,\n",
    "            kernel_size=self._kernel_size,\n",
    "            stride=self._stride,\n",
    "            repeat_num=self._sampling_num,\n",
    "        )\n",
    "        # b, t, f -> b, c, t, f\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        b, c, t, f = x.size()\n",
    "        # b, c, t, f -> b, t, c * f\n",
    "        x = x.transpose(1, 2).reshape(b, t, -1)\n",
    "        x = self.out(x)\n",
    "        return x, lengths\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def streaming_forward(self, x, state):\n",
    "        \"\"\"\n",
    "            x: torch.Tensor of shape [1, T, F]\n",
    "            state: dict\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee19ac7-c444-4ce3-b21e-bf3f7ca02297",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformerFeedForward(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_ff: int,\n",
    "        dropout: float,\n",
    "        activation: torch.nn.Module = torch.nn.SiLU()\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._d_model = d_model\n",
    "        self._d_ff = d_ff\n",
    "        self._dropout = dropout\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(self._d_model, self._d_ff)\n",
    "        self.activation = activation\n",
    "        self.dropout = torch.nn.Dropout(p=self._dropout)\n",
    "        self.linear2 = torch.nn.Linear(self._d_ff, self._d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CausalConv1D(torch.nn.Conv1d):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        stride: int,\n",
    "        groups: int = 1,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        self._left_padding = kernel_size - 1\n",
    "        self._right_padding = 0\n",
    "\n",
    "        super().__init__(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=0,\n",
    "            groups=groups,\n",
    "            bias=bias,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.pad(x, pad=(self._left_padding, self._right_padding))\n",
    "        return super().forward(x)\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def streaming_forward(self, x, state):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class ConformerConvolution(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        kernel_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert (kernel_size - 1) % 2 == 0\n",
    "        self._d_model = d_model\n",
    "        self._kernel_size = kernel_size\n",
    "        self.pointwise_activation = lambda x: torch.nn.functional.glu(x, dim=1)\n",
    "\n",
    "        self.pointwise_conv1 = torch.nn.Conv1d(\n",
    "            in_channels=self._d_model, out_channels=self._d_model * 2, kernel_size=1, stride=1, padding=0, bias=True\n",
    "        )\n",
    "        self.depthwise_conv = CausalConv1D(\n",
    "            in_channels=self._d_model,\n",
    "            out_channels=self._d_model,\n",
    "            kernel_size=self._kernel_size,\n",
    "            stride=1,\n",
    "            groups=self._d_model,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        # yep, batch_norm here is layer norm\n",
    "        self.batch_norm = torch.nn.LayerNorm(self._d_model)\n",
    "        self.activation = torch.nn.SiLU()\n",
    "        self.pointwise_conv2 = torch.nn.Conv1d(\n",
    "            in_channels=self._d_model, out_channels=d_model, kernel_size=1, stride=1, padding=0, bias=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x, pad_mask=None):\n",
    "        # x: [B, T, F]\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.pointwise_conv1(x)\n",
    "        x = self.pointwise_activation(x)\n",
    "\n",
    "        if pad_mask is not None:\n",
    "            x = x.float().masked_fill(pad_mask.unsqueeze(1), 0.0)\n",
    "\n",
    "        x = self.depthwise_conv(x)\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.batch_norm(x)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        x = self.activation(x)\n",
    "        x = self.pointwise_conv2(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def streaming_forward(self, x, state):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class ConformerLayer(torch.nn.Module):\n",
    "    \"\"\"A single block of the Conformer encoder.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): input dimension of MultiheadAttentionMechanism and PositionwiseFeedForward\n",
    "        d_ff (int): hidden dimension of PositionwiseFeedForward\n",
    "        n_heads (int): number of heads for multi-head attention\n",
    "        conv_kernel_size (int): kernel size for depthwise convolution in convolution module\n",
    "        dropout (float): dropout probabilities for linear layers\n",
    "        dropout_att (float): dropout probabilities for attention distributions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        d_ff: int,\n",
    "        n_heads: int,\n",
    "        conv_kernel_size: int,\n",
    "        dropout: float,\n",
    "        dropout_att: float,\n",
    "        # pos_bias_u: torch.nn.Parameter,\n",
    "        # pos_bias_v: torch.nn.Parameter,\n",
    "        att_context_size: tuple[int, int],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._d_model = d_model\n",
    "        self._d_ff = d_ff\n",
    "        self._n_heads = n_heads\n",
    "        self._conv_kernel_size = conv_kernel_size\n",
    "        self._dropout = dropout\n",
    "        self._dropout_att = dropout_att\n",
    "        self._att_context_size = att_context_size\n",
    "\n",
    "        self._fc_factor = 0.5\n",
    "\n",
    "        self.norm_feed_forward1 = torch.nn.LayerNorm(self._d_model)\n",
    "        self.feed_forward1 = ConformerFeedForward(d_model=self._d_model, d_ff=self._d_ff, dropout=self._dropout)\n",
    "        \n",
    "        self.norm_conv = torch.nn.LayerNorm(self._d_model)\n",
    "        self.conv = ConformerConvolution(\n",
    "            d_model=self._d_model,\n",
    "            kernel_size=self._conv_kernel_size,\n",
    "        )\n",
    "\n",
    "        # multi-headed self-attention module\n",
    "        self.norm_self_att = torch.nn.LayerNorm(self._d_model)\n",
    "\n",
    "        # TODO: Add RelPositionMultiHeadAttention\n",
    "        self.self_attn = RelPositionMultiHeadAttention(\n",
    "            n_head=self._n_heads,\n",
    "            n_feat=self._d_model,\n",
    "            dropout_rate=self._dropout_att,\n",
    "            # pos_bias_u=pos_bias_u,\n",
    "            # pos_bias_v=pos_bias_v\n",
    "        )\n",
    "\n",
    "        self.norm_feed_forward2 = torch.nn.LayerNorm(self._d_model)\n",
    "        self.feed_forward2 = ConformerFeedForward(d_model=self._d_model, d_ff=self._d_ff, dropout=self._dropout)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(self._dropout)\n",
    "        self.norm_out = torch.nn.LayerNorm(self._d_model)\n",
    "\n",
    "    def forward(self, x, att_mask=None, pos_emb=None, pad_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): input signals (B, T, d_model)\n",
    "            att_mask (torch.Tensor): attention masks(B, T, T)\n",
    "            pos_emb (torch.Tensor): (L, 1, d_model)\n",
    "            pad_mask (torch.tensor): padding mask (B, T)\n",
    "        Returns:\n",
    "            x (torch.Tensor): (B, T, d_model)\n",
    "            cache_last_channel (torch.tensor) : next cache for MHA layers (B, T_cache, d_model)\n",
    "            cache_last_time (torch.tensor) : next cache for convolutional layers (B, d_model, T_cache)\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x = self.norm_feed_forward1(x)\n",
    "        x = self.feed_forward1(x)\n",
    "        residual = residual + self.dropout(x) * self._fc_factor\n",
    "\n",
    "        x = self.norm_self_att(residual)\n",
    "        x = self.self_attn(query=x, key=x, value=x, mask=att_mask, pos_emb=pos_emb)\n",
    "\n",
    "        residual = residual + self.dropout(x)\n",
    "\n",
    "        x = self.norm_conv(residual)\n",
    "        x = self.conv(x, pad_mask=pad_mask)\n",
    "        residual = residual + self.dropout(x)\n",
    "\n",
    "        x = self.norm_feed_forward2(residual)\n",
    "        x = self.feed_forward2(x)\n",
    "        residual = residual + self.dropout(x) * self._fc_factor\n",
    "\n",
    "        x = self.norm_out(residual)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_initial_state(self, chunk_size, left_context_chunks):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def streaming_forward(self, x, pos_emb, state):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class ConformerEncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feat_in: int,\n",
    "        n_layers: int,\n",
    "        d_model: int,\n",
    "        ff_expansion_factor: int,\n",
    "        n_heads: int,\n",
    "        subsampling_factor: int,\n",
    "        subsampling_conv_channels: int,\n",
    "        att_context_size: tuple[int, int],\n",
    "        conv_kernel_size: int,\n",
    "        pos_emb_max_len: int = 5000,\n",
    "        dropout: float = 0.1,\n",
    "        dropout_pre_encoder: float = 0.1,\n",
    "        dropout_emb: float = 0.1,\n",
    "        dropout_att: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._feat_in = feat_in\n",
    "        self._n_layers = n_layers\n",
    "        self._d_model = d_model\n",
    "        self._ff_expansion_factor = ff_expansion_factor\n",
    "        self._n_heads = n_heads\n",
    "        self._subsampling_factor = subsampling_factor\n",
    "        self._subsampling_conv_channels = subsampling_conv_channels\n",
    "\n",
    "        self._x_scale = math.sqrt(self._d_model)\n",
    "        \n",
    "        self._att_context_size = att_context_size\n",
    "        self._conv_kernel_size = conv_kernel_size\n",
    "        self._conv_context_size = [conv_kernel_size - 1, 0]\n",
    "        self._pos_emb_max_len = pos_emb_max_len\n",
    "        self._dropout = dropout\n",
    "        self._dropout_pre_encoder = dropout_pre_encoder\n",
    "        self._dropout_emb = dropout_emb\n",
    "        self._dropout_att = dropout_att\n",
    "\n",
    "        self.pre_encode = ConvSubsampling(\n",
    "            subsampling_factor=self._subsampling_factor,\n",
    "            feat_in=self._feat_in,\n",
    "            feat_out=self._d_model,\n",
    "            conv_channels=self._subsampling_conv_channels,\n",
    "            activation=torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self._feat_out = d_model\n",
    "\n",
    "        self._d_head = self._d_model // self._n_heads\n",
    "\n",
    "\n",
    "        self.pos_enc = RelPositionalEncoding(\n",
    "            d_model=d_model,\n",
    "            dropout_rate=dropout_pre_encoder,\n",
    "            max_len=pos_emb_max_len,\n",
    "            xscale=self._x_scale,\n",
    "            dropout_rate_emb=dropout_emb,\n",
    "        )\n",
    "        self.pos_enc.extend_pe(pos_emb_max_len)\n",
    "\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(n_layers):\n",
    "            layer = ConformerLayer(\n",
    "                d_model=self._d_model,\n",
    "                d_ff=self._d_model * self._ff_expansion_factor,\n",
    "                n_heads=self._n_heads,\n",
    "                conv_kernel_size=self._conv_kernel_size,\n",
    "                dropout=self._dropout,\n",
    "                dropout_att=self._dropout_att,\n",
    "                att_context_size=self._att_context_size,\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def _create_masks(self, lengths, max_length, device):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Tuple of (pad_mask, att_mask)\n",
    "            pad_mask: torch.Tensor, (B, T), bool. True means value should not be used\n",
    "            att_mask: torch.Tensor, (B, T, T), bool. True means value should not be used\n",
    "        \"\"\"\n",
    "        chunk_size = self._att_context_size[1] + 1\n",
    "        \n",
    "        # left_chunks_num specifies the number of chunks to be visible by each chunk on the left side\n",
    "        left_chunks_num = self._att_context_size[0] // chunk_size\n",
    "\n",
    "        att_mask = create_attn_mask(chunk_size, left_chunks_num, max_length, device)\n",
    "\n",
    "        # pad_mask is the masking to be used to ignore paddings\n",
    "        # [b, t]: pad_mask[i, j] = lengths[i] <= j\n",
    "        pad_mask = torch.arange(0, max_length, device=device).expand(\n",
    "            lengths.size(0), -1\n",
    "        ) < lengths.unsqueeze(-1)\n",
    "\n",
    "        \n",
    "        # pad_mask_for_att_mask is the mask which helps to ignore paddings\n",
    "        # [b, t, t]\n",
    "        pad_mask_for_att_mask = pad_mask.unsqueeze(1).repeat([1, max_length, 1])\n",
    "        pad_mask_for_att_mask = torch.logical_and(pad_mask_for_att_mask, pad_mask_for_att_mask.transpose(1, 2))\n",
    "\n",
    "        # just in case\n",
    "        assert att_mask.shape[1] == max_length\n",
    "        assert att_mask.shape[2] == max_length\n",
    "        assert att_mask.device == pad_mask_for_att_mask.device\n",
    "\n",
    "        # paddings should also get ignored, so pad_mask_for_att_mask is used to ignore their corresponding scores\n",
    "        att_mask = torch.logical_and(pad_mask_for_att_mask, att_mask)\n",
    "        att_mask = ~att_mask\n",
    "        pad_mask = ~pad_mask\n",
    "        return pad_mask, att_mask\n",
    "\n",
    "    \n",
    "    def forward(self, features, lengths):\n",
    "        \"\"\"\n",
    "            features: [B, F, T]\n",
    "            lengths: [B]\n",
    "        \"\"\"\n",
    "        features = torch.transpose(features, 1, 2)\n",
    "        features, lengths = self.pre_encode(x=features, lengths=lengths)\n",
    "        lengths = lengths.to(torch.int64)\n",
    "        features, pos_emb = self.pos_enc(x=features)\n",
    "        pad_mask, att_mask = self._create_masks(\n",
    "            lengths=lengths,\n",
    "            max_length=features.size(1),\n",
    "            device=features.device,\n",
    "        )\n",
    "\n",
    "        for layer in self.layers:\n",
    "            features = layer(\n",
    "                x=features,\n",
    "                att_mask=att_mask,\n",
    "                pos_emb=pos_emb,\n",
    "                pad_mask=pad_mask,\n",
    "            )\n",
    "            # return features, lengths\n",
    "\n",
    "        features = torch.transpose(features, 1, 2)\n",
    "        return features, lengths\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def streaming_forward(self, features, state):\n",
    "        raise NotImplementedError()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b4b537-ad67-4527-9160-bd82f0a028fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CtcDecoder(torch.nn.Module):\n",
    "    def __init__(self, enc_output_size, tokenizer_settings):\n",
    "        super().__init__()\n",
    "        self._tokenizer_settings = tokenizer_settings\n",
    "        self.decoder_layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(\n",
    "                in_channels=enc_output_size,\n",
    "                out_channels=len(tokenizer_settings['token_to_piece']) + 1,\n",
    "                kernel_size=1,\n",
    "                stride=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, enc_output, enc_lengths):\n",
    "        return self.decoder_layers(enc_output), enc_lengths\n",
    "\n",
    "    def decode(self, logits):\n",
    "        # logits = logits.cpu().detach().numpy()\n",
    "        logits = logits.transpose(2, 1)\n",
    "        result = []\n",
    "        for idx in range(len(logits)):\n",
    "            tokens = list(map(int, logits[idx].max(dim=-1)[1].cpu().detach().numpy()))\n",
    "            prediction = []\n",
    "            prev_token = None\n",
    "            for token in tokens:\n",
    "                if token != prev_token and token != self._tokenizer_settings['blank_idx']:\n",
    "                    prediction.append(self._tokenizer_settings['token_to_piece'][str(token)])\n",
    "                prev_token = token\n",
    "            result.append(''.join(prediction).replace(self._tokenizer_settings['special_symbol'], ' '))\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71355b2-68e6-4385-a638-049c6597f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ConformerEncoder(\n",
    "    feat_in=80,\n",
    "    n_layers=17,\n",
    "    d_model=512,\n",
    "    ff_expansion_factor=4,\n",
    "    n_heads=8,\n",
    "    subsampling_factor=8,\n",
    "    subsampling_conv_channels=256,\n",
    "    att_context_size=[70, 1],\n",
    "    conv_kernel_size=9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7dc0a6-a13a-4490-a9eb-1e7728b1fede",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/week12_data/token.json') as fp:\n",
    "    tokenizer_settings = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91798aa-10ba-4292-8e97-14e437708a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = CtcDecoder(512, tokenizer_settings)\n",
    "decoder = decoder.cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee6a976-96d3-448d-ae52-65b40658dbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/week12_data/encoder_state.pkl', 'rb') as fp:\n",
    "    encoder.load_state_dict(pickle.load(fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d365e8-68c8-4464-8aa0-103ad4b34121",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/week12_data/decoder_state.pkl', 'rb') as fp:\n",
    "    decoder.load_state_dict(pickle.load(fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55397193-c588-4c60-976c-1e1229cea49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = encoder.cpu().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3a04f0-f15c-4ac4-867f-dcfaa06cbf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('../data/week12_data/audio.wav', 'rb') as fp:\n",
    "    with wave.open(fp, 'r') as wfp:\n",
    "        pcm_data = wfp.readframes(wfp.getnframes())\n",
    "\n",
    "signal = np.frombuffer(pcm_data, dtype=np.int16)\n",
    "signal = (signal.astype(np.float32) / 2. ** 15).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea5338-a465-4087-808a-05c2519ef7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio('../data/week12_data/audio.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1283c8-e6cc-43b8-877c-13c9413536ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_num_threads(1)\n",
    "# torch.set_num_interop_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16a1a58-6d5e-4728-851e-5315d67910ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = FilterbankFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1017640-ba31-47a1-9055-d6614f641e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    features = featurizer(torch.tensor(signal))\n",
    "    print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea27fa5-171d-4e64-87c7-bb5e06025fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    encoded, encoded_len = encoder(features.unsqueeze(0), torch.tensor([features.size(1)]))\n",
    "    print(encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda8d82-6481-4877-ad24-1bda350127a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits, logits_len = decoder(encoded, encoded_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b80dbc4-750b-4d25-a098-60de6fafb87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    decoder.decode(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b591879d-15f3-4ec9-9ab3-d15d29d6b5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b724cbc-643b-4245-9064-d1d00629d03d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae18655e-dc3a-4786-86b3-70adc62ea93d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
