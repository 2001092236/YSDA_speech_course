{"nbformat":4,"nbformat_minor":4,"metadata":{"kernelspec":{"name":"python3","display_name":"Yandex DataSphere Kernel","language":"python"},"language_info":{"file_extension":".py","pygments_lexer":"ipython3","nbconvert_exporter":"python","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","version":"3.7.7"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}},"notebookId":"ad46a522-5341-4555-ba42-195b12c06985"},"cells":[{"cell_type":"markdown","source":"# Seminar 3: CTC Forward-Backward and Greedy Decoder\n\n\nThe goal of this lab is to implement the Levenshtein Algorithm","metadata":{"cellId":"z5qfpgxviokgo9ewdahyxr"}},{"cell_type":"code","source":"#!L\nimport os\nimport numpy as np\nimport time\n\nfrom utils import *\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport torchaudio\nfrom torch import optim\nimport collections, math\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LogNorm","metadata":{"cellId":"0jg5sfs7qowv79473xsi8eu","trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Tokenizer Class","metadata":{"cellId":"abgm2usdt24l6itvepmw8"}},{"cell_type":"code","source":"#!L\n# Class to transform text to strings of token indecies\nclass Tokenizer:\n    \"\"\"Maps characters to integers and vice versa\"\"\"\n\n    def __init__(self):\n        char_map_str = \"\"\"\n        ' 0\n        _ 1\n        a 2\n        b 3\n        c 4\n        d 5\n        e 6\n        f 7\n        g 8\n        h 9\n        i 10\n        j 11\n        k 12\n        l 13\n        m 14\n        n 15\n        o 16\n        p 17\n        q 18\n        r 19\n        s 20\n        t 21\n        u 22\n        v 23\n        w 24\n        x 25\n        y 26\n        z 27\n        \"\"\"\n        self.char_map = {}\n        self.index_map = {}\n        for line in char_map_str.strip().split('\\n'):\n            ch, index = line.split()\n            self.char_map[ch] = int(index)\n            self.index_map[int(index)] = ch\n        self.index_map[1] = ' '\n\n    def text_to_indecies(self, text):\n        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n        int_sequence = []\n        for c in text:\n            if c == ' ':\n                ch = self.char_map['_']\n            else:\n                ch = self.char_map[c]\n            int_sequence.append(ch)\n        return int_sequence\n\n    def indecies_to_text(self, labels):\n        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n        string = []\n        for i in labels:\n            string.append(self.index_map[i])\n        return ''.join(string).replace('_', ' ')","metadata":{"cellId":"jarxy3gwdllxcqufdtz4f","trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Obtain Soft-Alignment via CTC Forward-Backward Algorithm (8 points)\n\nUse your newfound knowledge of the CTC forward-backward algorithm to obtain a soft-alignment\n\nRemember, that the forward variable is computed as follows:\n\n$$\n  \\begin{aligned}\n    &\\alpha_t(0) = 0, \\forall t & \\\\\n    &\\alpha_1(1) = {\\tt P}(z_1 = \\epsilon | \\mathbf{X}_{1:T}), &\\\\\n    &\\alpha_1(2) = {\\tt P}(z_1 = \\omega^{'}_2 | \\mathbf{X}_{1:T}), &\\\\\n    &\\alpha_1(s) = 0,\\ \\forall s > 2 &\\\\\n    &\\alpha_t(s) = 0,\\ \\forall s < (2L+1) -2(T-t) -1 &  \\text{top right zeros}\\\\\n    &\\alpha_t(s) = \\left \\{\n  \\begin{aligned}\n    &\\big(\\alpha_{t-1}(s) + \\alpha_{t-1}(s-1) \\big) {\\tt P}(z_t = \\omega^{'}_s | \\mathbf{X}_{1:T}) & \\text{if}\\ \\omega_s^{'} = \\epsilon\\ \\text{or}\\\n    \\omega_s^{'} = \\omega_{s-2}^{'} \\\\\n    &\\big(\\alpha_{t-1}(s) + \\alpha_{t-1}(s-1) + \\alpha_{t-1}(s-2)\\big) {\\tt P}(z_t = \\omega^{'}_s | \\mathbf{X}_{1:T}) & \\text{otherwise}\\\\\n  \\end{aligned} \\right. \n  \\end{aligned}\n$$\n\nBackward Variable is computeed like so:\n\n$$\n  \\begin{aligned}\n    &\\beta_T(2L+1) = 1 &\\\\\n    &\\beta_T(2L) = 1 & \\\\\n    &\\beta_T(s) = 0, \\forall s < 2L &\\\\\n    &\\beta_t(s) = 0,\\ \\forall s > 2t &\\\\\n    &\\beta_t(2L+2) = 0,\\ \\forall t  & \\text{Bottom left zeros} \\\\\n    &\\beta_t(s) = \\left \\{\n  \\begin{aligned}\n    &\\big(\\beta_{t+1}(s) + \\beta_{t+1}(s+1) \\big) {\\tt P}(z_t = \\omega^{'}_s | \\mathbf{X}_{1:T}) & \\text{if}\\ \\omega_s^{'} = \\epsilon\\ \\text{or}\\\n    \\omega_s^{'} = \\omega_{s+2}^{'} \\\\\n    &\\big(\\beta_{t+1}(s) +\\beta_{t+1}(s+1) + \\beta_{t+1}(s+2)\\big) {\\tt P}(z_t = \\omega^{'}_s | \\mathbf{X}_{1:T}) & \\text{otherwise}\\\\\n  \\end{aligned} \\right. \n  \\end{aligned}\n$$\n\nThe alignment is then given as:\n\n$$\n  \\text{align}_t(s) = \\frac{\\alpha_t(s)\\beta_t(s)}{\\sum_{s}\\alpha_t(s)\\beta_t(s)}\n$$\n\n\nDoing the computation in Probability space can be numerically unstable, so you should do it in Log-Space using the\nprovided logsumexp operation. Remember to return to prob space at the end. You should get something like:\n\n<img src=\"alignment.png\">\n\nIf you convert the alignment to log-space it will look like:\n\n<img src=\"alignment_log.png\">","metadata":{"cellId":"3lkqllygp9flmj29013zsg"}},{"cell_type":"code","source":"#!L\nfrom matplotlib.colors import LogNorm\n# Stable log sum exp\nNEG_INF = -float(\"inf\")\ndef logsumexp(*args):\n    \"\"\"\n    Stable log sum exp.\n    \"\"\"\n    if all(a == NEG_INF for a in args):\n        return NEG_INF\n    a_max = max(args)\n    lsp = math.log(sum(math.exp(a - a_max)\n                       for a in args))\n    return a_max + lsp\n\n\n\ndef forward_algorithm(sequence, matrix, blank=28):\n    # Matrix is has shape [K, T]\n    \n    # Turn probs into log-probs\n    matrix = np.log(matrix)\n    \n    # Create modified sequence which with START, END blanks and between each character\n    mod_sequence = [sequence[int((i-1)/2)] if i % 2 !=0 else blank for i in range(len(sequence)*2+1)]\n    \n    # Initialze\n    alphas = np.zeros([len(mod_sequence), matrix.shape[1]])\n    alphas[:,:]=NEG_INF = -float(\"inf\")\n    \n    for t in range(matrix.shape[1]):\n        for s in range(len(mod_sequence)):\n            \n            #First Step\n            if t == 0:\n                <YOUR CODE HERE>\n            \n            # Upper diagonal zeros\n            elif <YOUR CODE HERE>:\n                <YOUR CODE HERE>\n            else:\n                # Need to do this stabily\n                if s == 0:\n                    <YOUR CODE HERE>\n                    \n                elif s == 1:\n                    <YOUR CODE HERE> \n                    \n                else:\n                    <YOUR CODE HERE>\n                        \n    return alphas\n    \ndef backward_algorithm(sequence, matrix, blank=28):\n    # Matrix has shape [K, T] where K - 29 \n    \n    # Turn probs into log-probs\n    matrix = np.log(matrix)\n    \n    # Create modified sequence which with START, END blanks and between each character\n    mod_sequence = [sequence[int((i-1)/2)] if i % 2 !=0 else blank for i in range(len(sequence)*2+1)]\n    \n    betas = np.zeros([len(mod_sequence), matrix.shape[1]])\n    betas[:,:]=NEG_INF = -float(\"inf\")\n    \n    for t in reversed(range(matrix.shape[1])):\n        for s in reversed(range(len(mod_sequence))):\n            \n            #First Step\n            if t == matrix.shape[1]-1:\n                <YOUR CODE HERE>\n                \n            #Lower Diagonal Zeros\n            elif <YOUR CODE HERE>:\n                <YOUR CODE HERE>\n            else:\n                \n                if s == len(mod_sequence)-1:\n                   <YOUR CODE HERE>\n                \n                elif s == len(mod_sequence)-2:\n                    <YOUR CODE HERE>\n                    \n                else:\n                    <YOUR CODE HERE>\n                            \n    return betas\n\ndef soft_alignment(labels_indecies, matrix):\n    alphas = forward_algorithm(labels_indecies, matrix)\n    betas = backward_algorithm(labels_indecies, matrix)\n    \n    # Move from log space back to prob space\n    # align = <YOUR CODE>\n    align = np.exp(alphas+betas)\n    \n    # Normalize Alignment\n    align = <YOUR CODE>\n    \n    return align","metadata":{"cellId":"hz15vjonrhcd0goeoye94","trusted":true},"outputs":[],"execution_count":31},{"cell_type":"code","source":"#!L\n# Test your implementation\n\n#Load numpy matrix, add axis [classes,time]\nmatrix = np.loadtxt('test_matrix.txt')\n# Create label_sequence\nlabels_indecies = tokenizer.text_to_indecies('there se ms no good reason for believing that twillc ange')\n\nalign = soft_alignment(labels_indecies, matrix)\n\nplt.figure(dpi=150)\nplt.imshow(align, aspect='auto', interpolation='nearest')\nplt.colorbar()\n\nplt.figure(dpi=150)\nplt.imshow(np.log(align), aspect='auto', interpolation='nearest')\nplt.colorbar()\n\nref_align = np.loadtxt('soft_alignment.txt')\n\nassert np.all(ref_align == align)","metadata":{"cellId":"j0hk1mmq56mit6ldayht","trusted":true},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"## Implement a Greedy Best-Path Decoder! (2 points)\n\nYour goal is to implement a Greedy Best-Path decoder. Remember than in CTC the joint distribution over states factors out into a product of marginals:\n\n$${\\tt P}(\\mathbf{z}_{1:T}|\\mathbf{X}_{1:T},\\mathbf{\\theta}) = \\prod_{1:T}^T{\\tt P}(z_t|\\mathbf{X}_{1:T},\\mathbf{\\theta})$$\n\nMostly likely state sequence is:\n\n$$\\mathbf{\\pi}^*_{1:T} = \\arg \\max_{\\mathbf{pi}_{1:T} } \\prod_{t=1}^T {\\tt P}(z_t = \\pi_t|\\mathbf{X}_{1:T})$$\n\nThen merge repeats and remove blanks.","metadata":{"cellId":"eignaazwtkqbepk36j4zm"}},{"cell_type":"code","source":"#!L\ndef GreedyDecoder(output: torch.Tensor, labels: list, label_lengths: list, blank_label=28, collapse_repeated=True):\n    \"\"\"ist\n    :param output: torch.Tensor of Probs or Log-Probs of shape [batch, time, classes]\n    :param labels: Listof torch.Tensor of label indecies\n    :param label_lengths:  int of label lengths\n    :param blank_label: \n    :param collapse_repeated: \n    :return: \n    \"\"\"\n    \n    #Get max class\n    # arg_maxes = <YOUR_CODE>\n    arg_maxes = torch.argmax(output, dim=2)\n    \n    decodes = []\n    targets = []\n    \n    #For targets and decodes remove r\n    for i, args in enumerate(arg_maxes):\n        decode = []\n        \n        targets.append(tokenizer.indecies_to_text(labels[i][:label_lengths[i]].tolist()))\n        \n        # Remove repeeats, then remove blanks\n        \n        for j, index in enumerate(args):\n            if index != blank_label:\n                if collapse_repeated and j != 0 and index == args[j - 1]:\n                    continue\n                decode.append(index.item())\n        decodes.append(tokenizer.indecies_to_text(decode))\n    return decodes, targets","metadata":{"cellId":"jzbg8qm8vcrinevp9y0dok","trusted":true},"outputs":[],"execution_count":8},{"cell_type":"code","source":"#!L\n# TESTING THE GREEDY DECODER \n\n#Load numpy matrix, add axis [batch,classes,time]\nmatrix = np.loadtxt('test_matrix.txt')[np.newaxis,:,:]\n\n# Turn into Torch Tensor of shape [batch, time, classes]\nmatrix = torch.Tensor(matrix).transpose(1,2)\n\n# Create list of torch tensor\nlabels_indecies = [torch.Tensor(tokenizer.text_to_indecies('there seems no good reason for believing that it will change'))]\n\n# Run the Decoder\ndecodes, targets = GreedyDecoder(matrix, labels_indecies, [len(labels_indecies[0])])\n\nassert decodes[0] == 'there se ms no good reason for believing that twillc ange'\nassert targets[0] == 'there seems no good reason for believing that it will change'\n\n","metadata":{"cellId":"se7g7cudhf1r43b40icy9","trusted":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Seminar Ends and Homework Begins here","metadata":{"cellId":"gvc1msqg6viwm3iwt79xy"}},{"cell_type":"code","source":"#!L\n","metadata":{"cellId":"4i95bg5u0c4pj26vvkbxyc"},"outputs":[],"execution_count":null}]}